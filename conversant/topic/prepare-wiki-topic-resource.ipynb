{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, Tuple, List, Iterable, Sequence\n",
    "import time\n",
    "import subprocess\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, spmatrix\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "from sparselsh import LSH"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def count_lines(path: str) -> int:\n",
    "    command = f\"wc -l \\\"{path}\\\"\"\n",
    "    return int(subprocess.check_output(command, shell=True).split()[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process wiki token counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "WikiTokenCount = Tuple[str, str, int]\n",
    "\n",
    "def parse_wiki_token_count_line(line: str) -> WikiTokenCount:\n",
    "    wid, token, count = line.strip().split(',')\n",
    "    return wid, token, int(count)\n",
    "\n",
    "\n",
    "def iter_wiki_counts_file(path: str, n_lines: int = None) -> Iterable[WikiTokenCount]:\n",
    "    if n_lines is None:\n",
    "        print(\"compute number of lines in file\")\n",
    "        n_lines = count_lines(path)\n",
    "        print(f\"counted {n_lines} lines in file: {path}\")\n",
    "\n",
    "    with open(path, 'r', buffering=1024**2) as f:\n",
    "        lines_iter = tqdm(f, total=n_lines)\n",
    "        yield from map(parse_wiki_token_count_line, lines_iter)\n",
    "\n",
    "\n",
    "def process_wiki_counts(wiki_counts: List[WikiTokenCount], min_count: int = 5) -> Tuple[Dict[str, int], List[int], int]:\n",
    "    tokens_indices: Dict[str, int] = {}\n",
    "    tokens = []\n",
    "    tokens_docs_count: List[int] = []     # docs_counts_by_token_index\n",
    "    n_docs = 0\n",
    "    for doc_id, counts in groupby(wiki_counts, key=itemgetter(0)):\n",
    "        n_docs += 1\n",
    "        for (_, token, count) in counts:\n",
    "            token_i = tokens_indices.setdefault(token, len(tokens_indices))\n",
    "            if token_i == len(tokens_docs_count):\n",
    "                tokens_docs_count.append(1)\n",
    "                tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            tokens_docs_count[token_i] += 1\n",
    "\n",
    "    freq_token_indices = [i for i, token_count in enumerate(tokens_docs_count) if token_count >= min_count]\n",
    "    tokens = [tokens[i] for i in freq_token_indices]\n",
    "    tokens_docs_count = [tokens_docs_count[i] for i in freq_token_indices]\n",
    "    tokens_indices = {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "    return tokens_indices, tokens_docs_count, n_docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=458462175.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11b7648b2e4b4ca2a8e61c8ec504bb78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished parsing 4334806 docs, with 918912 unique tokens\n"
     ]
    }
   ],
   "source": [
    "wiki_count_path = \"/Users/ronpick/studies/stance/data/wikipedia-tfidf/tfidf/wp_word_count.csv\"\n",
    "# wiki_idf_path = \"/Users/ronpick/studies/stance/data/wikipedia-tfidf/tfidf/wp_word_idfs.csv\"\n",
    "\n",
    "n_lines = count_lines(wiki_count_path)\n",
    "count_lines = iter_wiki_counts_file(wiki_count_path, n_lines=n_lines)\n",
    "token2index, tokens_docs_count, n_docs = process_wiki_counts(count_lines)\n",
    "print(f\"Finished parsing {n_docs} docs, with {len(token2index)} unique tokens\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def filter_tokens_by_min_count(\n",
    "        tokens_indices: Dict[str, int],\n",
    "        tokens_docs_count: List[int],\n",
    "        min_count: int = 5\n",
    ") -> Tuple[Dict[str, int], List[int]]:\n",
    "    tokens = map(itemgetter(0), sorted(tokens_indices.items(), key=itemgetter(1)))\n",
    "\n",
    "    freq_token_indices = [i for i, token_count in enumerate(tokens_docs_count) if token_count >= min_count]\n",
    "    tokens = [tokens[i] for i in freq_token_indices]\n",
    "    tokens_docs_count = [tokens_docs_count[i] for i in freq_token_indices]\n",
    "    tokens_indices = {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "    return tokens_indices, tokens_docs_count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute Count vectors of tokens for each document"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def iter_count_vectors_batches(\n",
    "        wiki_counts: List[WikiTokenCount],\n",
    "        tokens_indices: Dict[str, int],\n",
    "        batch_size: int = 10_000\n",
    ") -> Tuple[csc_matrix, List[str]]:\n",
    "    n_tokens = len(tokens_indices)  # number of cols\n",
    "    docs, tokens, data, doc_ids = [], [], [], []\n",
    "    for doc_id, counts in groupby(wiki_counts, key=itemgetter(0)):\n",
    "\n",
    "        if len(doc_ids) == batch_size:\n",
    "            yield csc_matrix((data, (docs, tokens)), shape=(batch_size, n_tokens)), doc_ids\n",
    "            docs, tokens, data, doc_ids = [], [], [], []\n",
    "\n",
    "        doc_ids.append(doc_id)\n",
    "        for _, token, count in counts:\n",
    "            token_index = tokens_indices.get(token)\n",
    "            if token_index is not None:\n",
    "                docs.append(len(doc_ids) - 1)\n",
    "                tokens.append(token_index)\n",
    "                data.append(count)\n",
    "\n",
    "    yield csc_matrix((data, (docs, tokens)), shape=(len(doc_ids), n_tokens)), doc_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute TF-IDF vectors into a sparse matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "docs_counts = np.array(tokens_docs_count).astype(np.float32)\n",
    "idf = np.log(n_docs * np.reciprocal(docs_counts)).reshape(1, -1)\n",
    "\n",
    "def compute_tfidf_batches(\n",
    "        count_vectors_batches: Iterable[Tuple[spmatrix, List[str]]],\n",
    "        docs_counts: List[int],\n",
    "        n_docs: int\n",
    ") -> Iterable[Tuple[spmatrix, List[str]]]:\n",
    "    docs_counts = np.array(docs_counts).astype(np.float32)\n",
    "    idf = np.log(n_docs * np.reciprocal(docs_counts)).reshape(1, -1)\n",
    "    for count_vectors, doc_ids in count_vectors_batches:\n",
    "        yield count_vectors.multiply(idf), doc_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "batch_size = 50_000\n",
    "count_lines = iter_wiki_counts_file(wiki_count_path, n_lines=n_lines)\n",
    "count_vecs_batches = iter_count_vectors_batches(count_lines, token2index, batch_size=batch_size)\n",
    "tfidf_batches = compute_tfidf_batches(count_vecs_batches, tokens_docs_count, n_docs=n_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perfrom LSH"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<sparselsh.lsh.LSH at 0x1565ce250>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh = LSH(\n",
    "    8,\n",
    "    len(token2index),\n",
    "    num_hashtables=1,\n",
    "    storage_config={\"dict\":None}\n",
    ")\n",
    "\n",
    "lsh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def hash_docs(m: spmatrix, lsh: LSH) -> np.ndarray:\n",
    "    planes = lsh.uniform_planes[0]\n",
    "    return planes.dot(m.T).T.toarray() > 0\n",
    "\n",
    "\n",
    "def encode_docs(m: spmatrix, lsh: LSH) -> Sequence[int]:\n",
    "    planes = lsh.uniform_planes[0]\n",
    "    hashed = planes.dot(m.T).toarray() > 0\n",
    "    packed = np.zeros(hashed.shape[1], dtype=np.uint32)\n",
    "    for i in range(hashed.shape[0]):\n",
    "        packed = packed << 1 | hashed[i]\n",
    "\n",
    "    return packed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# tfidf, doc_ids = next(tfidf_batches)\n",
    "# tfidf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# planes = lsh.uniform_planes[0]\n",
    "# hashed = planes.dot(tfidf.T).toarray() > 0\n",
    "# hashed\n",
    "# packed = np.zeros(hashed.shape[1], dtype=np.uint32)\n",
    "# print(packed.shape)\n",
    "# for i in range(hashed.shape[0]):\n",
    "#     packed = packed << 1 | hashed[i]\n",
    "#\n",
    "# packed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=458462175.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "559b620796c940bab1d01837b588e6ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "concat data\n"
     ]
    },
    {
     "data": {
      "text/plain": "(4334806, 8)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_batches = []\n",
    "all_doc_ids = []\n",
    "for tfidf, doc_ids in tfidf_batches:\n",
    "    tfidf_csr = tfidf.tocsr()\n",
    "    hashed_docs = hash_docs(tfidf_csr, lsh)\n",
    "    hashed_batches.append(hashed_docs)\n",
    "    all_doc_ids.extend(doc_ids)\n",
    "\n",
    "print(\"concat data\")\n",
    "data = np.vstack(hashed_batches)\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 1, ..., 1, 0, 0],\n       [1, 0, 1, ..., 1, 0, 1],\n       [0, 0, 0, ..., 1, 1, 1],\n       ...,\n       [1, 1, 0, ..., 1, 1, 1],\n       [1, 1, 0, ..., 1, 0, 0],\n       [1, 1, 0, ..., 0, 1, 0]], dtype=uint8)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.astype(np.uint8)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "tree = AnnoyIndex(8, \"hamming\")\n",
    "for i in tqdm(range(data.shape[0]), total=data.shape[0]):\n",
    "    tree.add_item(i, data[i])\n",
    "\n",
    "print(\"Building annoy tree\")\n",
    "tree.build(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10_000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 4.77 µs\n",
      "Create KD-Tree\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRecursionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-3759eaeb783d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'time'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Create KD-Tree\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtree\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKDTree\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mleafsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, leafsize)\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmins\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mamin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtree\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaxes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m     \u001B[0;32mclass\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[1;32m    324\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 325\u001B[0;31m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[0m\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    327\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__query\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdistance_upper_bound\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001B[0;32m~/.virtualenvs/conversant/lib/python3.8/site-packages/scipy/spatial/kdtree.py\u001B[0m in \u001B[0;36m__build\u001B[0;34m(self, idx, maxes, mins)\u001B[0m\n\u001B[1;32m    322\u001B[0m             \u001B[0mgreatermins\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m             return KDTree.innernode(d, split,\n\u001B[0;32m--> 324\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__build\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mless_idx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlessmaxes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmins\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    325\u001B[0m                     self.__build(idx[greater_idx],maxes,greatermins))\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRecursionError\u001B[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print(\"Create KD-Tree\")\n",
    "tree = KDTree(data, leafsize=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ascii_upper_start = 65\n",
    "ascii_upper_end = 90\n",
    "ascii_lower_start = 97\n",
    "ascii_lower_end = 122\n",
    "\n",
    "def is_ascii_letter(c: str) -> bool:\n",
    "    i = ord(c)\n",
    "    if ascii_upper_start <= i <= ascii_upper_end:\n",
    "        return True\n",
    "\n",
    "    return ascii_lower_start <= i <= ascii_lower_end\n",
    "\n",
    "\n",
    "def get_longest_letters_sequence(token: str) -> str:\n",
    "    end, best_len, current_len = 0, 0, 0\n",
    "    for i in range(len(token)):\n",
    "        if not is_ascii_letter(token[i]):\n",
    "            if current_len > best_len:\n",
    "                end = i\n",
    "                best_len = current_len\n",
    "\n",
    "            current_len = 0\n",
    "            continue\n",
    "\n",
    "        current_len += 1\n",
    "\n",
    "    return token[end-best_len: end]\n",
    "\n",
    "\n",
    "def is_valid_token(token: str) -> bool:\n",
    "    return len(token) > 2\n",
    "\n",
    "\n",
    "def tokenize_clean_text(text: str) -> List[str]:\n",
    "    tokens = map(get_longest_letters_sequence, text.split())\n",
    "    return list(filter(is_valid_token, tokens))\n",
    "\n",
    "\n",
    "def get_tfidf_vector(text: str, token2index: Dict[str, int], idf: np.ndarray, ) -> spmatrix:\n",
    "    tokens = tokenize_clean_text(text)\n",
    "    counts = Counter(filter(bool, map(token2index.get, tokens)))\n",
    "    rows = np.zeros(len(counts))\n",
    "    cols, data = sorted(counts.items(), key=itemgetter(0))\n",
    "    counts_vec = csc_matrix((data, (rows, cols)), shape=(1, len(token2index)))\n",
    "    return counts_vec.multiply(idf)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"Guns should be banned because they are not needed in any domestic issue.\" \\\n",
    "       \"The second ammendment was put in place because of fear that the british might invade america again or take \" \\\n",
    "       \"control of the government. \" \\\n",
    "       \"if this were the case the people would need weapons to defend themselves and regain america. \" \\\n",
    "       \"The british aren't going to invade so we don't need to protect our selves. \" \\\n",
    "       \"even in the this day and age america remains increadible safe compared to many other nations. \" \\\n",
    "       \"we have no close enemies. \" \\\n",
    "       \"if a major army were to attack us a few men with pistols or shotguns wouldn't do much against a soldier with \" \\\n",
    "       \"an ak47 or tanks or bombersGuns in America just make it easier for crimes to be committed. \" \\\n",
    "       \"Some guns should never be considered allowed and this includes all semi automatic weapons as well as \" \\\n",
    "       \"shotgunsPoverty, drugs, and lack of education are the reasons people turn to guns to kill. \" \\\n",
    "       \"guns give you power to take life and should not be allowed to float around so that our students or citizens \" \\\n",
    "       \"can use them against one another\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenize_clean_text(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_repr = get_tfidf_vector(text, token2index, idf)\n",
    "hashed_text = hash_docs(text_repr, lsh)\n",
    "tree.query(hashed_text, k=10, )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encoded_docs = encode_docs(tfidf_csr, lsh)\n",
    "# start = time.time()\n",
    "# tfidf_dense = tfidf.toarray()\n",
    "# for i, (doc_hash, doc_id) in tqdm(enumerate(zip(doc_ids, doc_ids)), total=len(doc_ids), leave=False):\n",
    "#     storage.append_val(doc_hash, (tfidf_dense[0], doc_id))\n",
    "# duration = time.time() - start\n",
    "# print(f\"done encoding {batch_size} docs into lsh [{duration} sec]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}