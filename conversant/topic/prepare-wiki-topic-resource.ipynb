{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, Tuple, List, Iterable, Sequence, Union\n",
    "import time\n",
    "import subprocess\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "from scipy.sparse import csc_matrix, spmatrix, csr_matrix, save_npz, load_npz\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "from sparselsh import LSH"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_lines(path: str) -> int:\n",
    "    command = f\"wc -l \\\"{path}\\\"\"\n",
    "    return int(subprocess.check_output(command, shell=True).split()[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process wiki token counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WikiTokenCount = Tuple[str, str, int]\n",
    "\n",
    "def parse_wiki_token_count_line(line: str) -> WikiTokenCount:\n",
    "    wid, token, count = line.strip().split(',')\n",
    "    return wid, token, int(count)\n",
    "\n",
    "\n",
    "def iter_wiki_counts_file(path: str, n_lines: int = None) -> Iterable[WikiTokenCount]:\n",
    "    if n_lines is None:\n",
    "        print(\"compute number of lines in file\")\n",
    "        n_lines = count_lines(path)\n",
    "        print(f\"counted {n_lines} lines in file: {path}\")\n",
    "\n",
    "    with open(path, 'r', buffering=1024**2) as f:\n",
    "        lines_iter = tqdm(f, total=n_lines)\n",
    "        yield from map(parse_wiki_token_count_line, lines_iter)\n",
    "\n",
    "\n",
    "def process_wiki_counts(wiki_counts: List[WikiTokenCount], min_count: int = 5) -> Tuple[Dict[str, int], List[int], int]:\n",
    "    tokens_indices: Dict[str, int] = {}\n",
    "    tokens = []\n",
    "    tokens_docs_count: List[int] = []     # docs_counts_by_token_index\n",
    "    n_docs = 0\n",
    "    for doc_id, counts in groupby(wiki_counts, key=itemgetter(0)):\n",
    "        n_docs += 1\n",
    "        for (_, token, count) in counts:\n",
    "            token_i = tokens_indices.setdefault(token, len(tokens_indices))\n",
    "            if token_i == len(tokens_docs_count):\n",
    "                tokens_docs_count.append(1)\n",
    "                tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            tokens_docs_count[token_i] += 1\n",
    "\n",
    "    freq_token_indices = [i for i, token_count in enumerate(tokens_docs_count) if token_count >= min_count]\n",
    "    tokens = [tokens[i] for i in freq_token_indices]\n",
    "    tokens_docs_count = [tokens_docs_count[i] for i in freq_token_indices]\n",
    "    tokens_indices = {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "    return tokens_indices, tokens_docs_count, n_docs\n",
    "\n",
    "\n",
    "def filter_tokens_by_min_count(\n",
    "        tokens_indices: Dict[str, int],\n",
    "        tokens_docs_count: List[int],\n",
    "        min_count: int = 5\n",
    ") -> Tuple[Dict[str, int], List[int]]:\n",
    "    tokens = map(itemgetter(0), sorted(tokens_indices.items(), key=itemgetter(1)))\n",
    "\n",
    "    freq_token_indices = [i for i, token_count in enumerate(tokens_docs_count) if token_count >= min_count]\n",
    "    tokens = [tokens[i] for i in freq_token_indices]\n",
    "    tokens_docs_count = [tokens_docs_count[i] for i in freq_token_indices]\n",
    "    tokens_indices = {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "    return tokens_indices, tokens_docs_count\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wiki_count_path = \"/Users/ronpick/studies/stance/data/wikipedia-tfidf/tfidf/wp_word_count.csv\"\n",
    "# # wiki_idf_path = \"/Users/ronpick/studies/stance/data/wikipedia-tfidf/tfidf/wp_word_idfs.csv\"\n",
    "#\n",
    "n_lines = count_lines(wiki_count_path)\n",
    "# wiki_lines = iter_wiki_counts_file(wiki_count_path, n_lines=n_lines)\n",
    "# token2index, tokens_docs_count, n_docs = process_wiki_counts(wiki_lines)\n",
    "# print(f\"Finished parsing {n_docs} docs, with {len(token2index)} unique tokens\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_token_doc_counts(tokens_indices: Dict[str, int], tokens_docs_count: List[int], n_docs: int, path: str):\n",
    "    tokens = list(map(itemgetter(0), sorted(tokens_indices.items(), key=itemgetter(1))))\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(f\"*,{n_docs}\")\n",
    "        for (token, count) in zip(tokens, tokens_docs_count):\n",
    "            f.write(f\"\\n{token},{count}\")\n",
    "\n",
    "\n",
    "def load_token_doc_counts(path: str) -> Tuple[Dict[str, int], List[int], int]:\n",
    "    with open(path, 'r') as f:\n",
    "        n_docs = int(next(f).split(',')[1])\n",
    "        split_lines = map(lambda l: l.split(','), f)\n",
    "        tokens, counts = list(zip(*[(token, int(count)) for token, count in split_lines]))\n",
    "        tokens_indices = {token: i for i, token in enumerate(tokens)}\n",
    "        return tokens_indices, list(counts), n_docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_df_path = \"tokens_df.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save_token_doc_counts(token2index, tokens_docs_count, n_docs, token_df_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token2index, tokens_docs_count, n_docs = load_token_doc_counts(token_df_path)\n",
    "tokens = list(map(itemgetter(0), sorted(token2index.items(), key=itemgetter(1))))\n",
    "print(f\"Loaded data for {n_docs} docs, with {len(tokens)} unique tokens\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating IDF values\n",
    "idf value for each token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs_counts = np.array(tokens_docs_count).astype(np.float32)\n",
    "idf = np.log(n_docs * np.reciprocal(docs_counts)).reshape(1, -1)\n",
    "idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## compute TF-IDF vectors into a sparse matrix\n",
    "- Compute Count vectors of tokens for each document\n",
    "- multiply each count vector by the idf values (element-wise)\n",
    "- Do that in batches for memory efficiency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def iter_count_vectors_batches(\n",
    "        wiki_counts: List[WikiTokenCount],\n",
    "        tokens_indices: Dict[str, int],\n",
    "        batch_size: int = 10_000,\n",
    "        norm: bool = True\n",
    ") -> Tuple[csc_matrix, List[str]]:\n",
    "    n_tokens = len(tokens_indices)  # number of cols\n",
    "    docs, tokens, data, doc_ids = [], [], [], []\n",
    "    for doc_id, counts in groupby(wiki_counts, key=itemgetter(0)):\n",
    "\n",
    "        if len(doc_ids) == batch_size:\n",
    "            yield csc_matrix((data, (docs, tokens)), shape=(batch_size, n_tokens)), doc_ids\n",
    "            docs, tokens, data, doc_ids = [], [], [], []\n",
    "\n",
    "        doc_ids.append(doc_id)\n",
    "        current_counts = []\n",
    "        for _, token, count in counts:\n",
    "            token_index = tokens_indices.get(token)\n",
    "            if token_index is not None:\n",
    "                docs.append(len(doc_ids) - 1)\n",
    "                tokens.append(token_index)\n",
    "                current_counts.append(count)\n",
    "\n",
    "        sum_counts = sum(current_counts) if norm else 1\n",
    "        [data.append(c / sum_counts) for c in current_counts]\n",
    "\n",
    "    yield csc_matrix((data, (docs, tokens)), shape=(len(doc_ids), n_tokens)), doc_ids\n",
    "\n",
    "\n",
    "def compute_tfidf_batches(\n",
    "        count_vectors_batches: Iterable[Tuple[spmatrix, List[str]]],\n",
    "        idfs: np.ndarray\n",
    ") -> Iterable[Tuple[spmatrix, List[str]]]:\n",
    "    for count_vectors, doc_ids in count_vectors_batches:\n",
    "        yield count_vectors.multiply(idfs), doc_ids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_size = n_docs + 1\n",
    "# count_lines = iter_wiki_counts_file(wiki_count_path, n_lines=n_lines)\n",
    "# count_vecs_batches = iter_count_vectors_batches(count_lines, token2index, batch_size=batch_size)\n",
    "# tfidf_batches = compute_tfidf_batches(count_vecs_batches, idf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tfidf, all_doc_ids = next(tfidf_batches)\n",
    "# print(\"convert to csr format\")\n",
    "# tfidf = tfidf.tocsr()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_doc_ids(doc_ids: List[str], path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        for doc_id in doc_ids:\n",
    "            f.write(f\"{doc_id}\\n\")\n",
    "\n",
    "\n",
    "def load_doc_ids(path: str) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        return list(map(str.strip, f))\n",
    "\n",
    "\n",
    "def save_sparse_matrix(path: str, m: csr_matrix):\n",
    "    np.savez_compressed(path, indptr=m.indptr, indices=m.indices, data=m.data)\n",
    "\n",
    "\n",
    "def load_sparse_matrix(path: str, shape: Tuple[int, int]) -> csr_matrix:\n",
    "    arrays = np.load(path, allow_pickle=False)\n",
    "    indptr = arrays[\"indptr\"]\n",
    "    indices = arrays[\"indices\"]\n",
    "    data = arrays[\"data\"]\n",
    "    return csr_matrix((data, indices, indptr), shape=shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_ids_path = \"docids.txt\"\n",
    "docs_repr_path = \"docs_repr.npz\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save_doc_ids(all_doc_ids, doc_ids_path)\n",
    "# save_sparse_matrix(docs_repr_path, tfidf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_doc_ids = load_doc_ids(doc_ids_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf = load_sparse_matrix(docs_repr_path, shape=(n_docs, len(token2index)))\n",
    "tfidf\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import normalize\n",
    "#\n",
    "# tfidf_norm = normalize(tfidf, norm='l2', axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perfrom LSH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_hyperplanes(n_hashes: int, rng: np.random.Generator) -> csr_matrix:\n",
    "    total_size = n_hashes * len(token2index)\n",
    "    planes = np.ones(total_size)\n",
    "    planes[:total_size//2] = -1\n",
    "    planes = rng.permutation(planes).reshape(n_hashes, len(token2index))\n",
    "    return csr_matrix(planes)\n",
    "\n",
    "def compute_signatures(m: spmatrix, planes: np.ndarray) -> np.ndarray:\n",
    "    return (planes.dot(m.transpose()).toarray() > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def compute_buckets(signatures: np.ndarray, hashes_per_bucket: int = 20) -> np.ndarray:\n",
    "    n_hashes, n_docs = signatures.shape\n",
    "    buckets = []\n",
    "    i = 0\n",
    "    while i < n_hashes:\n",
    "        sub_signatures = signatures[i: i + hashes_per_bucket]\n",
    "        print(f\"Calculate buckets for signatures of length {len(sub_signatures)}: {i} - {i + hashes_per_bucket}\")\n",
    "        packed = np.zeros(n_docs, dtype=np.uint32)\n",
    "        for j in range(len(sub_signatures)):\n",
    "            packed = packed << 1 | sub_signatures[j]\n",
    "\n",
    "        buckets.append(packed)\n",
    "        i += hashes_per_bucket\n",
    "\n",
    "    buckets = np.vstack(buckets)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def create_hashtables(buckets_per_doc: Sequence[Sequence[int]]) -> List[Dict[int, List[int]]]:\n",
    "    all_buckets_mapping = []\n",
    "    for buckets in buckets_per_doc:\n",
    "        buckets_map = {}\n",
    "        for doc_i, h in enumerate(buckets):\n",
    "            docs = buckets_map.setdefault(h, [])\n",
    "            docs.append(doc_i)\n",
    "\n",
    "        all_buckets_mapping.append(buckets_map)\n",
    "\n",
    "    return all_buckets_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crteate random hyperplanes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# num_hashes = 100\n",
    "# rng = np.random.default_rng(1919)\n",
    "# planes = generate_hyperplanes(num_hashes, rng)\n",
    "# planes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### calculate docs signatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done - [took 1072.1158928871155 sec]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[1, 1, 0, ..., 0, 1, 1],\n       [1, 0, 0, ..., 0, 1, 1],\n       [0, 1, 1, ..., 0, 0, 0],\n       ...,\n       [0, 1, 0, ..., 1, 0, 0],\n       [1, 1, 1, ..., 1, 0, 0],\n       [0, 1, 0, ..., 1, 0, 1]], dtype=uint8)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"calculating hashes\")\n",
    "# start = time.time()\n",
    "# signatures = compute_signatures(tfidf, planes)\n",
    "# duration = time.time() - start\n",
    "# print(f\"Done - [took {duration} sec]\")\n",
    "# signatures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store LSH data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lsh_data_path = \"lsh.npz\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# np.savez_compressed(lsh_data_path, planes=planes.toarray(), signatures=signatures)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = np.load(lsh_data_path)\n",
    "planes = csr_matrix(data[\"planes\"])\n",
    "signatures = data[\"signatures\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### pack signatures to int hashes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "buckets_per_doc = compute_buckets(signatures, hashes_per_bucket=10)\n",
    "buckets_per_doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create hash-tables for each"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%time\n",
    "hashtables = create_hashtables(buckets_per_doc)\n",
    "len(hashtables)\n",
    "for ht in hashtables:\n",
    "    print(len(ht))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "page = wikipedia.page(pageid=all_doc_ids[232251])\n",
    "print(page.pageid, page.title)\n",
    "#\n",
    "# page = wikipedia.page(pageid=all_doc_ids[2537209])\n",
    "# print(page.pageid, page.title)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r1 = tfidf[36]\n",
    "print(r1.indices)\n",
    "r2 = tfidf[2537209]\n",
    "print(r2.indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = list(map(itemgetter(0), sorted(token2index.items(), key=itemgetter(1))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tokens_docs_count[8264])\n",
    "tokens[8264], np.log(n_docs / tokens_docs_count[8264])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r1.toarray()[0][8264], r2.toarray()[0][8264]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hashed_batches = []\n",
    "# all_doc_ids = []\n",
    "# for tfidf, doc_ids in tfidf_batches:\n",
    "#     tfidf_csr = tfidf.tocsr()\n",
    "#     hashed_docs = hash_docs(tfidf_csr, lsh)\n",
    "#     hashed_batches.append(hashed_docs)\n",
    "#     all_doc_ids.extend(doc_ids)\n",
    "#\n",
    "# print(\"concat data\")\n",
    "# data = np.vstack(hashed_batches)\n",
    "# data.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "n_docs = data.shape[0]\n",
    "dim = data.shape[1]\n",
    "tree = AnnoyIndex(dim, \"hamming\")\n",
    "for i in tqdm(range(n_docs), total=n_docs):\n",
    "    tree.add_item(i, data[i])\n",
    "\n",
    "print(\"Building annoy tree\")\n",
    "tree.build(100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ascii_lower_start = 97\n",
    "ascii_lower_end = 122\n",
    "\n",
    "def is_ascii_letter(c: str) -> bool:\n",
    "    return ascii_lower_start <= ord(c) <= ascii_lower_end\n",
    "\n",
    "\n",
    "def get_longest_letters_sequence(token: str) -> str:\n",
    "    token = token.lower()\n",
    "    end, best_len, current_len = 0, 0, 0\n",
    "    for i in range(len(token)):\n",
    "        if not is_ascii_letter(token[i]):\n",
    "            if current_len > best_len:\n",
    "                end = i\n",
    "                best_len = current_len\n",
    "\n",
    "            current_len = 0\n",
    "            continue\n",
    "\n",
    "        current_len += 1\n",
    "\n",
    "    if current_len > best_len:\n",
    "        return token[-current_len:]\n",
    "    return token[end-best_len: end].lower()\n",
    "\n",
    "\n",
    "def is_valid_token(token: str) -> bool:\n",
    "    return len(token) > 2\n",
    "\n",
    "\n",
    "def tokenize_clean_text(text: str) -> List[str]:\n",
    "    tokens = map(get_longest_letters_sequence, text.split())\n",
    "    return list(filter(is_valid_token, tokens))\n",
    "\n",
    "\n",
    "def get_tfidf_vector(text: str, token2index: Dict[str, int], idf: np.ndarray, ) -> csr_matrix:\n",
    "    tokens = tokenize_clean_text(text)\n",
    "    counts = Counter(filter(bool, map(token2index.get, tokens)))\n",
    "    rows = np.zeros(len(counts))\n",
    "    cols, data = zip(*sorted(counts.items(), key=itemgetter(0)))\n",
    "    counts_vec = csc_matrix((data, (rows, cols)), shape=(1, len(token2index)))\n",
    "    return counts_vec.multiply(idf).tocsr()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"Guns should be banned because they are not needed in any domestic issue.\" \\\n",
    "       \"The second ammendment was put in place because of fear that the british might invade america again or take \" \\\n",
    "       \"control of the government. \" \\\n",
    "       \"if this were the case the people would need weapons to defend themselves and regain america. \" \\\n",
    "       \"The british aren't going to invade so we don't need to protect our selves. \" \\\n",
    "       \"even in the this day and age america remains increadible safe compared to many other nations. \" \\\n",
    "       \"we have no close enemies. \" \\\n",
    "       \"if a major army were to attack us a few men with pistols or shotguns wouldn't do much against a soldier with \" \\\n",
    "       \"an ak47 or tanks or bombersGuns in America just make it easier for crimes to be committed. \" \\\n",
    "       \"Some guns should never be considered allowed and this includes all semi automatic weapons as well as \" \\\n",
    "       \"shotgunsPoverty, drugs, and lack of education are the reasons people turn to guns to kill. \" \\\n",
    "       \"guns give you power to take life and should not be allowed to float around so that our students or citizens \" \\\n",
    "       \"can use them against one another\"\n",
    "\n",
    "text2 = \"\"\"Vasily Ivanovich Chuikov (Russian: Васи́лий Ива́нович Чуйко́в; About this soundlisten (help·info); 12 February [O.S. 31 January] 1900 – 18 March 1982) was a Soviet military commander and Marshal of the Soviet Union. He is best known for commanding the 62nd Army which saw heavy combat during the Battle of Stalingrad in the Second World War.\n",
    "\n",
    "Born to a peasant family near Tula, Chuikov earned his living as a factory worker from the age of 12. After the Russian Revolution of 1917, he joined the Red Army and distinguished himself during the Russian Civil War. After graduating from the Frunze Military Academy, Chuikov worked as a military attaché and intelligence officer in China and the Russian Far East. At the outbreak of the Second World War, Chuikov commanded the 4th Army during the Soviet invasion of Poland, and the 9th Army during the Winter War against Finland. In December 1940, he was again appointed military attaché to China in support of Chiang Kai-shek and the Nationalists in the war against Japan.\n",
    "\n",
    "In March 1942, Chuikov was recalled from China to command the 62nd Army in defense of Stalingrad. Tasked with holding the city at all costs, Chuikov adopted the hugging tactic, keeping the Soviet front-line positions as close to the Germans as physically possible. This served as an effective countermeasure against the Wehrmacht's combined-arms tactics, but by mid-November 1942 the Germans had captured most of the city after months of slow advance. In late November Chuikov's 62nd Army joined the rest of the Soviet forces in a counter-offensive, which led to the surrender of the German army in early 1943. After Stalingrad, Chuikov led his forces into Poland during Operation Bagration and the Vistula–Oder Offensive before advancing on Berlin. He personally accepted the unconditional surrender of German forces in Berlin on 2 May 1945.\n",
    "\n",
    "After the war, Chuikov served as Chief of the Group of Soviet Forces in Germany (1949–53), commander of the Kiev Military District (1953–60), Chief of the Soviet Armed Forces and Deputy Minister of Defense (1960–64), and head of the Soviet Civil Defense Forces (1961–72). Chuikov was twice awarded the titles Hero of the Soviet Union (1944 and 1945) and was awarded the Distinguished Service Cross by the United States for his actions during the Battle of Stalingrad. In 1955, he was named a Marshal of the Soviet Union. Following his death in 1982, Chuikov was interred at the Stalingrad memorial at Mamayev Kurgan, which had been the site of heavy fighting.\"\"\"\n",
    "\n",
    "text3 = \"\"\"Mark Rutherford School is a mixed secondary school and sixth form in Bedford, England. The school is named in honour of the Bedford-born writer William Hale White (1831-1913), who used Mark Rutherford as a pseudonym.\n",
    "\n",
    "Mark Rutherford school educates pupils from age 11 through to 16. In addition, the school offers a sixth form provision for pupils age 16 to 19 wishing to study courses such as A levels. The school has a specialism in performing arts, and offers a range of courses related to the specialism.\"\"\"\n",
    "\n",
    "text4 = \"\"\"Abortion is the ending of a pregnancy by removal or expulsion of an embryo or fetus.[note 1] An abortion that occurs without intervention is known as a miscarriage or \"spontaneous abortion\" and occurs in approximately 30% to 40% of pregnancies.[1][2] When deliberate steps are taken to end a pregnancy, it is called an induced abortion, or less frequently \"induced miscarriage\". The unmodified word abortion generally refers to an induced abortion.[3][4]\n",
    "\n",
    "When properly done, abortion is one of the safest procedures in medicine,[5]:1 [6]:1 but unsafe abortion is a major cause of maternal death, especially in the developing world,[7] while making safe abortion legal and accessible reduces maternal deaths.[8][9] It is safer than childbirth, which has a 14 times higher risk of death in the United States.[10]\n",
    "\n",
    "Modern methods use medication or surgery for abortions.[11] The drug mifepristone in combination with prostaglandin appears to be as safe and effective as surgery during the first and second trimester of pregnancy.[11][12] The most common surgical technique involves dilating the cervix and using a suction device.[13] Birth control, such as the pill or intrauterine devices, can be used immediately following abortion.[12] When performed legally and safely on a woman who desires it, induced abortions do not increase the risk of long-term mental or physical problems.[14] In contrast, unsafe abortions (those performed by unskilled individuals, with hazardous equipment, or in unsanitary facilities) cause 47,000 deaths and 5 million hospital admissions each year.[14][15] The World Health Organization states that \"access to legal, safe and comprehensive abortion care, including post-abortion care, is essential for the attainment of the highest possible level of sexual and reproductive health\".[16]\n",
    "\n",
    "Around 56 million abortions are performed each year in the world,[17] with about 45% done unsafely.[18] Abortion rates changed little between 2003 and 2008,[19] before which they decreased for at least two decades as access to family planning and birth control increased.[20] As of 2018, 37% of the world's women had access to legal abortions without limits as to reason.[21][22] Countries that permit abortions have different limits on how late in pregnancy abortion is allowed.[22] Abortion rates are similar between countries that ban abortion and countries that allow it.[23]\n",
    "\n",
    "Historically, abortions have been attempted using herbal medicines, sharp tools, forceful massage, or through other traditional methods.[24] Abortion laws and cultural or religious views of abortions are different around the world. In some areas abortion is legal only in specific cases such as rape, problems with the fetus, poverty, risk to a woman's health, or incest.[25] There is debate over the moral, ethical, and legal issues of abortion.[26][27] Those who oppose abortion often argue that an embryo or fetus is a human with a right to life, and they may compare abortion to murder.[28][29] Those who support the legality of abortion often hold that it is part of a woman's right to make decisions about her own body.[30] Others favor legal and accessible abortion as a public health measure.[31]\"\"\"\n",
    "\n",
    "text5 = \"\"\"Same-sex marriage, also known as gay marriage, is the marriage of two people of the same sex or gender, entered into in a civil or religious ceremony. There are records of same-sex marriage dating back to the first century. In the modern era, marriage equality was first granted to same-sex couples in the Netherlands on 1 April 2001.\n",
    "\n",
    "As of 2021, same-sex marriage is legally performed and recognized in 29 countries (nationwide or in some jurisdictions):\n",
    "\n",
    "Argentina\n",
    "Australia\n",
    "Austria\n",
    "Belgium\n",
    "Brazil\n",
    "Canada\n",
    "Colombia\n",
    "Costa Rica\n",
    "Denmark\n",
    "Ecuador\n",
    "Finland\n",
    "France\n",
    "Germany\n",
    "Iceland\n",
    "Ireland\n",
    "Luxembourg\n",
    "Malta\n",
    "Mexico[a]\n",
    "Netherlands[b]\n",
    "New Zealand[c]\n",
    "Norway\n",
    "Portugal\n",
    "South Africa\n",
    "Spain\n",
    "Sweden\n",
    "Taiwan\n",
    "United Kingdom[d]\n",
    "United States[e]\n",
    "Uruguay\n",
    "The introduction of same-sex marriage (also called marriage equality) has varied by jurisdiction, and came about through legislative change to marriage law, court rulings based on constitutional guarantees of equality, recognition that it is allowed by existing marriage law,[1] or by direct popular vote (via referendums and initiatives). The recognition of same-sex marriage is considered to be a human right and a civil right as well as a political, social, and religious issue.[2] The most prominent supporters of same-sex marriage are human rights and civil rights organizations as well as the medical and scientific communities, while the most prominent opponents are religious fundamentalist groups. Polls consistently show continually rising support for the recognition of same-sex marriage in all developed democracies and in some developing democracies.\n",
    "\n",
    "Scientific studies show that the financial, psychological, and physical well-being of gay people are enhanced by marriage, and that the children of same-sex parents benefit from being raised by married same-sex couples within a marital union that is recognized by law and supported by societal institutions.[3] Social science research indicates that the exclusion of homosexuals from marriage stigmatizes and invites public discrimination against them, with research also repudiating the notion that either civilization or viable social orders depend upon restricting marriage to heterosexuals.[4] Same-sex marriage can provide those in committed same-sex relationships with relevant government services and make financial demands on them comparable to that required of those in opposite-sex marriages, and also gives them legal protections such as inheritance and hospital visitation rights.[5] Opposition to same-sex marriage is based on claims such as that homosexuality is unnatural and abnormal, that the recognition of same-sex unions will promote homosexuality in society, and that children are better off when raised by opposite-sex couples.[6] These claims are refuted by scientific studies, which show that homosexuality is a natural and normal variation in human sexuality, and that sexual orientation is not a choice. Many studies have shown that children of same-sex couples fare just as well as the children of opposite-sex couples; some studies have shown benefits to being raised by same-sex couples.[7]\n",
    "\n",
    "A study of nationwide data from across the United States from January 1999 to December 2015 revealed that the establishment of same-sex marriage is associated with a significant reduction in the rate of attempted suicide among children, with the effect being concentrated among children of a minority sexual orientation, resulting in about 134,000 fewer children attempting suicide each year in the United States.[8]\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_repr = get_tfidf_vector(text5, token2index, idf)\n",
    "cols = text_repr.indices\n",
    "t = [tokens[col] for col in cols]\n",
    "sorted((zip(t, text_repr.data)), key=itemgetter(1), reverse=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_signatures = compute_signatures(text_repr, planes)\n",
    "test_buckets = compute_buckets(test_signatures, hashes_per_bucket=10)\n",
    "test_buckets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs_counts = Counter()\n",
    "for i, b in enumerate(test_buckets):\n",
    "    docs_counts.update(hashtables[i].get(b[0], []))\n",
    "\n",
    "docs_counts.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn = list(map(itemgetter(0), docs_counts.most_common(10)))\n",
    "distances = [(i, text_repr.dot(tfidf[i].transpose()).toarray()[0][0], cosine(text_repr.toarray(), tfidf[i].toarray())) for i in nn]\n",
    "distances = sorted(distances, key=itemgetter(1), reverse=True)\n",
    "for d in distances:\n",
    "    print(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, dot, cos in distances:\n",
    "    pageid = all_doc_ids[i]\n",
    "    try:\n",
    "        page = wikipedia.page(pageid=pageid)\n",
    "        print(i, page.pageid, page.title)\n",
    "        print(cos, dot)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        print(pageid, e.options)\n",
    "    except wikipedia.PageError:\n",
    "        print(pageid, \"Doesn't exist\")\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encoded_docs = encode_docs(tfidf_csr, lsh)\n",
    "# start = time.time()\n",
    "# tfidf_dense = tfidf.toarray()\n",
    "# for i, (doc_hash, doc_id) in tqdm(enumerate(zip(doc_ids, doc_ids)), total=len(doc_ids), leave=False):\n",
    "#     storage.append_val(doc_hash, (tfidf_dense[0], doc_id))\n",
    "# duration = time.time() - start\n",
    "# print(f\"done encoding {batch_size} docs into lsh [{duration} sec]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}